{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "This notebook shows one the most basic RL algorithms - [REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf).  \n",
    "\n",
    "It was tested on the `CartPole-v1` only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Tuple\n",
    "\n",
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "\n",
    "from base import agent_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Policy and the Agent\n",
    "Policy can be some ML model which takes decisions based on the environment state (or the observation if the state is hidden).  \n",
    "Agent defines how:\n",
    "1. policy is created (`get_policy` method)\n",
    "2. action are taken (`take_action` method)\n",
    "3. policy is updated (`update_policy` method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, inputs_no: int, output_no: int, discrete_outputs: bool):\n",
    "        super().__init__()\n",
    "        self.discrete_outputs = discrete_outputs\n",
    "        self.fc1 = nn.Linear(inputs_no, 256)\n",
    "        self.dropout = nn.Dropout(p=0.8)\n",
    "        self.fc2 = nn.Linear(256, output_no)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        if self.discrete_outputs:\n",
    "            x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PolicyAgent(agent_base.Agent):\n",
    "    def __init__(self, env: gym.core.Env, gamma: float = 0.9, lr=0.01, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        :param: gamma: discount factor used to calculate return\n",
    "        :param: lr: learning rate used in the torch optimizer\n",
    "        \"\"\"\n",
    "        super().__init__(env, gamma, lr, *args, **kwargs)\n",
    "        self.observations = []\n",
    "        self.log_action_probabilities = []\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "    def take_action(self, observation: np.array, *args, **kwargs):\n",
    "        del args, kwargs  # unused\n",
    "\n",
    "        observation = torch.from_numpy(observation).float().unsqueeze(0)\n",
    "        probabilities = self.policy(observation)\n",
    "\n",
    "        m = Categorical(probabilities)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        self.log_action_probabilities.append(log_prob)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def get_policy(self) -> nn.Module:\n",
    "        inputs_no = self.get_observations()\n",
    "        outputs_no = self.get_actions()\n",
    "        discrete_outputs = self.discrete_actions\n",
    "        return Policy(inputs_no, outputs_no, discrete_outputs)\n",
    "\n",
    "    def update_policy(self, rewards: torch.tensor, *args, **kwargs):\n",
    "        del args, kwargs  # unused\n",
    "\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        returns = self.calculate_returns(rewards)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(self.log_action_probabilities, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        del self.log_action_probabilities[:]\n",
    "\n",
    "    def calculate_returns(self, rewards: torch.tensor) -> torch.tensor:\n",
    "        returns = torch.flip(rewards, [0])\n",
    "        for idx, item in enumerate(returns):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            returns[idx] = item + self.gamma * returns[idx - 1]\n",
    "        return torch.flip(returns, [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the environmnet and the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = PolicyAgent(env, gamma=0.9, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(\n",
    "    agent: agent_base.Agent, env: gym.Env\n",
    ") -> Tuple[agent_base.Agent, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform one simulation (episode) to collect data for the policy update.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    rewards = torch.Tensor(0, 1).float()\n",
    "\n",
    "    for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "        action = agent(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        rewards = torch.cat([rewards, torch.tensor(reward).view(1, 1)])\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return agent, rewards\n",
    "\n",
    "\n",
    "def print_epiode_stats(\n",
    "    i_episode: int,\n",
    "    epiosde_reward: float,\n",
    "    running_reward: float,\n",
    "    env: gym.Env,\n",
    "    rewards: torch.Tensor,\n",
    "):\n",
    "    if i_episode % 50 == 0:\n",
    "        print(\n",
    "            \"Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}\\t\".format(\n",
    "                i_episode, ep_reward, running_reward\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        t = rewards.size()[0]\n",
    "        print(\n",
    "            \"Solved - epiosde {}! Running reward is now {} and \"\n",
    "            \"the last episode runs to {} time steps!\".format(\n",
    "                i_episode, running_reward, t\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast reward: 10.00\tAverage reward: 10.00\t\n",
      "Episode 50\tLast reward: 105.00\tAverage reward: 78.98\t\n",
      "Episode 100\tLast reward: 122.00\tAverage reward: 222.01\t\n",
      "Episode 150\tLast reward: 167.00\tAverage reward: 193.61\t\n",
      "Episode 200\tLast reward: 500.00\tAverage reward: 301.65\t\n",
      "Episode 250\tLast reward: 140.00\tAverage reward: 207.47\t\n",
      "Episode 300\tLast reward: 272.00\tAverage reward: 240.10\t\n",
      "Episode 350\tLast reward: 126.00\tAverage reward: 180.75\t\n",
      "Episode 400\tLast reward: 125.00\tAverage reward: 217.01\t\n",
      "Episode 450\tLast reward: 116.00\tAverage reward: 132.87\t\n",
      "Episode 500\tLast reward: 159.00\tAverage reward: 98.61\t\n",
      "Episode 550\tLast reward: 164.00\tAverage reward: 153.14\t\n",
      "Episode 600\tLast reward: 201.00\tAverage reward: 306.77\t\n",
      "Episode 650\tLast reward: 135.00\tAverage reward: 168.96\t\n",
      "Episode 700\tLast reward: 208.00\tAverage reward: 342.04\t\n",
      "Episode 750\tLast reward: 347.00\tAverage reward: 341.34\t\n",
      "Episode 800\tLast reward: 113.00\tAverage reward: 165.69\t\n",
      "Episode 850\tLast reward: 228.00\tAverage reward: 182.59\t\n",
      "Episode 900\tLast reward: 192.00\tAverage reward: 229.79\t\n",
      "Episode 950\tLast reward: 245.00\tAverage reward: 337.24\t\n",
      "Episode 1000\tLast reward: 155.00\tAverage reward: 201.31\t\n",
      "Episode 1050\tLast reward: 194.00\tAverage reward: 192.00\t\n",
      "Episode 1100\tLast reward: 184.00\tAverage reward: 217.73\t\n",
      "Episode 1150\tLast reward: 188.00\tAverage reward: 210.50\t\n",
      "Episode 1200\tLast reward: 123.00\tAverage reward: 124.14\t\n",
      "Episode 1250\tLast reward: 209.00\tAverage reward: 173.11\t\n",
      "Episode 1300\tLast reward: 97.00\tAverage reward: 112.53\t\n",
      "Episode 1350\tLast reward: 156.00\tAverage reward: 171.18\t\n",
      "Episode 1400\tLast reward: 177.00\tAverage reward: 185.55\t\n",
      "Episode 1450\tLast reward: 208.00\tAverage reward: 187.20\t\n",
      "Episode 1500\tLast reward: 500.00\tAverage reward: 361.12\t\n",
      "Episode 1550\tLast reward: 276.00\tAverage reward: 315.97\t\n",
      "Episode 1600\tLast reward: 99.00\tAverage reward: 145.97\t\n",
      "Episode 1650\tLast reward: 118.00\tAverage reward: 118.20\t\n",
      "Episode 1700\tLast reward: 210.00\tAverage reward: 157.89\t\n",
      "Episode 1750\tLast reward: 140.00\tAverage reward: 170.73\t\n",
      "Episode 1800\tLast reward: 428.00\tAverage reward: 216.46\t\n",
      "Episode 1850\tLast reward: 464.00\tAverage reward: 419.66\t\n",
      "Episode 1900\tLast reward: 158.00\tAverage reward: 190.63\t\n",
      "Episode 1950\tLast reward: 199.00\tAverage reward: 168.63\t\n",
      "Episode 2000\tLast reward: 161.00\tAverage reward: 163.81\t\n",
      "Episode 2050\tLast reward: 195.00\tAverage reward: 220.91\t\n",
      "Episode 2100\tLast reward: 500.00\tAverage reward: 387.06\t\n",
      "Solved - epiosde 2130! Running reward is now 475.7584760314054 and the last episode runs to 500 time steps!\n"
     ]
    }
   ],
   "source": [
    "running_reward = None\n",
    "for i_episode in range(4000):\n",
    "    agent, rewards = run_episode(agent, env)\n",
    "    ep_reward = rewards.sum().item()\n",
    "\n",
    "    if running_reward is None:\n",
    "        running_reward = ep_reward\n",
    "    else:\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    print_epiode_stats(i_episode, ep_reward, running_reward, env, rewards)\n",
    "\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        break\n",
    "\n",
    "    agent.update_policy(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "for i in range(300):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    action = agent(obs)\n",
    "    obs, rew, done, info = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
