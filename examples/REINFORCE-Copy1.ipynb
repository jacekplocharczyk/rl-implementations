{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "This notebook shows one the most basic RL algorithms - [REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf).  \n",
    "\n",
    "It was tested on the `CartPole-v1` only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Tuple\n",
    "\n",
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "\n",
    "from base import agent_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Policy and the Agent\n",
    "Policy can be some ML model which takes decisions based on the environment state (or the observation if the state is hidden).  \n",
    "Agent defines how:\n",
    "1. policy is created (`get_policy` method)\n",
    "2. action are taken (`take_action` method)\n",
    "3. policy is updated (`update_policy` method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, inputs_no: int, output_no: int, discrete_outputs: bool):\n",
    "        super().__init__()\n",
    "        self.discrete_outputs = discrete_outputs\n",
    "        self.fc1 = nn.Linear(inputs_no, 256)\n",
    "        self.dropout = nn.Dropout(p=0.8)\n",
    "        self.fc2 = nn.Linear(256, output_no)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        if self.discrete_outputs:\n",
    "            x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PolicyAgent(agent_base.Agent):\n",
    "    def __init__(self, env: gym.core.Env, gamma: float = 0.9, lr=0.01, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        :param: gamma: discount factor used to calculate return\n",
    "        :param: lr: learning rate used in the torch optimizer\n",
    "        \"\"\"\n",
    "        super().__init__(env, gamma, lr, *args, **kwargs)\n",
    "        self.observations = []\n",
    "        self.log_action_probabilities = []\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "    def take_action(self, observation: np.array, *args, **kwargs):\n",
    "        del args, kwargs  # unused\n",
    "\n",
    "        observation = torch.from_numpy(observation).float().unsqueeze(0)\n",
    "        probabilities = self.policy(observation)\n",
    "\n",
    "        m = Categorical(probabilities)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        self.log_action_probabilities.append(log_prob)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def get_policy(self) -> nn.Module:\n",
    "        inputs_no = self.get_observations()\n",
    "        outputs_no = self.get_actions()\n",
    "        discrete_outputs = self.discrete_actions\n",
    "        return Policy(inputs_no, outputs_no, discrete_outputs)\n",
    "\n",
    "    def update_policy(self, rewards: torch.tensor, *args, **kwargs):\n",
    "        del args, kwargs  # unused\n",
    "\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        returns = self.calculate_returns(rewards)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(self.log_action_probabilities, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        del self.log_action_probabilities[:]\n",
    "\n",
    "    def calculate_returns(self, rewards: torch.tensor) -> torch.tensor:\n",
    "        returns = torch.flip(rewards, [0])\n",
    "        for idx, item in enumerate(returns):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            returns[idx] = item + self.gamma * returns[idx - 1]\n",
    "        return torch.flip(returns, [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the environmnet and the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = PolicyAgent(env, gamma=0.9, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(\n",
    "    agent: agent_base.Agent, env: gym.Env\n",
    ") -> Tuple[agent_base.Agent, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform one simulation (episode) to collect data for the policy update.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    rewards = torch.Tensor(0, 1).float()\n",
    "\n",
    "    for t in range(1, 100):  # Don't infinite loop while learning\n",
    "        action = agent(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            continue\n",
    "        rewards = torch.cat([rewards, torch.tensor(reward).view(1, 1)])\n",
    "\n",
    "\n",
    "\n",
    "    return agent, rewards\n",
    "\n",
    "\n",
    "def print_epiode_stats(\n",
    "    i_episode: int,\n",
    "    epiosde_reward: float,\n",
    "    running_reward: float,\n",
    "    env: gym.Env,\n",
    "    rewards: torch.Tensor,\n",
    "):\n",
    "    if i_episode % 50 == 0:\n",
    "        print(\n",
    "            \"Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}\\t\".format(\n",
    "                i_episode, ep_reward, running_reward\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        t = rewards.size()[0]\n",
    "        print(\n",
    "            \"Solved - epiosde {}! Running reward is now {} and \"\n",
    "            \"the last episode runs to {} time steps!\".format(\n",
    "                i_episode, running_reward, t\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 598 ms, sys: 953 ms, total: 1.55 s\n",
      "Wall time: 220 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "agent = PolicyAgent(env, gamma=0.9, lr=0.01)\n",
    "running_reward = None\n",
    "for i_episode in range(10):\n",
    "    agent, rewards = run_episode(agent, env)\n",
    "    ep_reward = rewards.sum().item()\n",
    "\n",
    "    if running_reward is None:\n",
    "        running_reward = ep_reward\n",
    "    else:\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "#     print_epiode_stats(i_episode, ep_reward, running_reward, env, rewards)\n",
    "\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        break\n",
    "\n",
    "    agent.update_policy(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast reward: 17.00\tAverage reward: 17.00\t\n",
      "Episode 50\tLast reward: 50.00\tAverage reward: 37.61\t\n",
      "Episode 100\tLast reward: 137.00\tAverage reward: 117.13\t\n",
      "Episode 150\tLast reward: 171.00\tAverage reward: 124.02\t\n",
      "Episode 200\tLast reward: 128.00\tAverage reward: 145.58\t\n",
      "Episode 250\tLast reward: 148.00\tAverage reward: 151.46\t\n",
      "Episode 300\tLast reward: 73.00\tAverage reward: 126.90\t\n",
      "Episode 350\tLast reward: 328.00\tAverage reward: 298.47\t\n",
      "Episode 400\tLast reward: 81.00\tAverage reward: 132.39\t\n",
      "Episode 450\tLast reward: 345.00\tAverage reward: 161.68\t\n",
      "Episode 500\tLast reward: 125.00\tAverage reward: 208.39\t\n",
      "Episode 550\tLast reward: 500.00\tAverage reward: 240.05\t\n",
      "Episode 600\tLast reward: 85.00\tAverage reward: 138.87\t\n",
      "Episode 650\tLast reward: 220.00\tAverage reward: 150.39\t\n",
      "Episode 700\tLast reward: 449.00\tAverage reward: 334.31\t\n",
      "Episode 750\tLast reward: 406.00\tAverage reward: 384.89\t\n",
      "Episode 800\tLast reward: 175.00\tAverage reward: 228.01\t\n",
      "Episode 850\tLast reward: 392.00\tAverage reward: 200.33\t\n",
      "Episode 900\tLast reward: 295.00\tAverage reward: 366.49\t\n",
      "Episode 950\tLast reward: 500.00\tAverage reward: 467.14\t\n",
      "Solved - epiosde 956! Running reward is now 475.84341731915464 and the last episode runs to 500 time steps!\n"
     ]
    }
   ],
   "source": [
    "running_reward = None\n",
    "for i_episode in range(4000):\n",
    "    agent, rewards = run_episode(agent, env)\n",
    "    ep_reward = rewards.sum().item()\n",
    "\n",
    "    if running_reward is None:\n",
    "        running_reward = ep_reward\n",
    "    else:\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    print_epiode_stats(i_episode, ep_reward, running_reward, env, rewards)\n",
    "\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        break\n",
    "\n",
    "    agent.update_policy(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "for i in range(300):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    action = agent(obs)\n",
    "    obs, rew, done, info = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
