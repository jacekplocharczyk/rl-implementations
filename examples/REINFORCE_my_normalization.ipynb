{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "This notebook shows one the most basic RL algorithms - [REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf).  \n",
    "\n",
    "It was tested on the `CartPole-v1` only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Tuple\n",
    "\n",
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from push_ups import agent_base\n",
    "from push_ups.utils import default_network, training_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Policy and the Agent\n",
    "Policy can be some ML model which takes decisions based on the environment state (or the observation if the state is hidden).  \n",
    "Agent defines how:\n",
    "1. policy is created (`get_policy` method)\n",
    "2. action are taken (`take_action` method)\n",
    "3. policy is updated (`update_policy` method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAgent(agent_base.Agent):\n",
    "    def __init__(self, env: gym.core.Env, gamma: float = 0.9, lr=0.01, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        :param: gamma: discount factor used to calculate return\n",
    "        :param: lr: learning rate used in the torch optimizer\n",
    "        \"\"\"\n",
    "        super().__init__(env, gamma, lr, *args, **kwargs)\n",
    "        self.observations = []\n",
    "        self.log_action_probabilities = []\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "    def take_action(self, observation: np.array, *args, **kwargs):\n",
    "        del args, kwargs  # unused\n",
    "        if not self.discrete_actions:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        observation = torch.from_numpy(observation).float().unsqueeze(0)\n",
    "        probabilities = self.policy(observation)\n",
    "\n",
    "        m = Categorical(probabilities)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        self.log_action_probabilities.append(log_prob)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def get_policy(self) -> nn.Module:\n",
    "        inputs_no = self.get_observations()\n",
    "        outputs_no = self.get_actions()\n",
    "        discrete_outputs = self.discrete_actions\n",
    "        return default_network.Policy(inputs_no, outputs_no, discrete_outputs)\n",
    "\n",
    "    def update_policy(self, rewards: torch.tensor, *args, **kwargs):\n",
    "        del args, kwargs  # unused\n",
    "\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        returns = self.calculate_returns(rewards)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(self.log_action_probabilities, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        del self.log_action_probabilities[:]\n",
    "\n",
    "    def calculate_returns(self, rewards: torch.tensor) -> torch.tensor:\n",
    "        returns = torch.flip(rewards, [0])\n",
    "        for idx, item in enumerate(returns):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            returns[idx] = item + self.gamma * returns[idx - 1]\n",
    "        return torch.flip(returns, [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the environmnet and the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = PolicyAgent(env, gamma=0.9, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast reward: 17.00\tAverage reward: 17.00\t\n",
      "Episode 50\tLast reward: 32.00\tAverage reward: 42.90\t\n",
      "Episode 100\tLast reward: 102.00\tAverage reward: 96.83\t\n",
      "Episode 150\tLast reward: 246.00\tAverage reward: 250.67\t\n",
      "Episode 200\tLast reward: 134.00\tAverage reward: 168.69\t\n",
      "Episode 250\tLast reward: 140.00\tAverage reward: 125.81\t\n",
      "Episode 300\tLast reward: 141.00\tAverage reward: 118.80\t\n",
      "Episode 350\tLast reward: 197.00\tAverage reward: 140.37\t\n",
      "Episode 400\tLast reward: 85.00\tAverage reward: 312.28\t\n",
      "Episode 450\tLast reward: 196.00\tAverage reward: 198.34\t\n",
      "Episode 500\tLast reward: 400.00\tAverage reward: 294.28\t\n",
      "Episode 550\tLast reward: 71.00\tAverage reward: 102.47\t\n",
      "Episode 600\tLast reward: 112.00\tAverage reward: 82.68\t\n",
      "Episode 650\tLast reward: 224.00\tAverage reward: 216.12\t\n",
      "Episode 700\tLast reward: 102.00\tAverage reward: 120.36\t\n",
      "Episode 750\tLast reward: 325.00\tAverage reward: 236.46\t\n",
      "Episode 800\tLast reward: 261.00\tAverage reward: 275.50\t\n",
      "Episode 850\tLast reward: 500.00\tAverage reward: 410.81\t\n",
      "Episode 900\tLast reward: 193.00\tAverage reward: 357.98\t\n",
      "Episode 950\tLast reward: 226.00\tAverage reward: 282.65\t\n",
      "Episode 1000\tLast reward: 239.00\tAverage reward: 200.24\t\n",
      "Episode 1050\tLast reward: 177.00\tAverage reward: 267.90\t\n",
      "Episode 1100\tLast reward: 190.00\tAverage reward: 169.57\t\n",
      "Episode 1150\tLast reward: 153.00\tAverage reward: 145.81\t\n",
      "Episode 1200\tLast reward: 234.00\tAverage reward: 219.24\t\n",
      "Episode 1250\tLast reward: 500.00\tAverage reward: 371.25\t\n",
      "Episode 1300\tLast reward: 271.00\tAverage reward: 387.37\t\n",
      "Episode 1350\tLast reward: 227.00\tAverage reward: 260.56\t\n",
      "Episode 1400\tLast reward: 147.00\tAverage reward: 169.61\t\n",
      "Episode 1450\tLast reward: 124.00\tAverage reward: 118.25\t\n",
      "Episode 1500\tLast reward: 215.00\tAverage reward: 162.29\t\n",
      "Episode 1550\tLast reward: 120.00\tAverage reward: 152.78\t\n",
      "Episode 1600\tLast reward: 161.00\tAverage reward: 142.22\t\n",
      "Episode 1650\tLast reward: 135.00\tAverage reward: 150.25\t\n",
      "Episode 1700\tLast reward: 135.00\tAverage reward: 159.08\t\n",
      "Episode 1750\tLast reward: 309.00\tAverage reward: 202.15\t\n",
      "Episode 1800\tLast reward: 500.00\tAverage reward: 470.30\t\n",
      "Solved - epiosde 1812! Running reward is now 476.0118659655744 and the last episode runs to 500 time steps!\n"
     ]
    }
   ],
   "source": [
    "running_reward = None\n",
    "for i_episode in range(4000):\n",
    "    agent, rewards = training_loop.run_episode(agent, env)\n",
    "    ep_reward = rewards.sum().item()\n",
    "\n",
    "    if running_reward is None:\n",
    "        running_reward = ep_reward\n",
    "    else:\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    training_loop.print_epiode_stats(i_episode, ep_reward, running_reward, env, rewards)\n",
    "\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        break\n",
    "\n",
    "    agent.update_policy(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "for i in range(300):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    action = agent(obs)\n",
    "    obs, rew, done, info = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
