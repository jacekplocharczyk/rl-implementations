{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "This notebook shows one the most basic RL algorithms - [REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=$PYTHONPATH:/projects/rl-implementations\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from base import agent_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Policy and the Agent\n",
    "Policy can be some ML model which takes decisions based on the environment state (or the observation if the state is hidden).  \n",
    "Agent defines how:\n",
    "1. policy is created (`get_policy` method)\n",
    "2. action are taken (`take_action` method)\n",
    "3. policy is updated (`update_policy` method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, inputs_no: int, output_no: int, discrete_outputs: bool):\n",
    "        super().__init__()\n",
    "        self.discrete_outputs = discrete_outputs\n",
    "        self.fc1 = nn.Linear(inputs_no, 256)\n",
    "        self.dropout = nn.Dropout(p=0.8)\n",
    "        self.fc2 = nn.Linear(256, output_no)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        if self.discrete_outputs:\n",
    "            x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class PolicyAgent(agent_base.Agent):\n",
    "    def __init__(self, gamma: float, lr: float, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        :param: gamma: discount factor used to calculate return\n",
    "        :param: lr: learning rate used in the torch optimizer\n",
    "        \"\"\"\n",
    "        super().__init__(gamma, *args, **kwargs)        \n",
    "        self.observations = []\n",
    "        self.log_action_probabilities = []\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "    \n",
    "    def take_action(self, observation: np.array, *args, **kwargs):\n",
    "        del args, kwargs  # unused\n",
    "        \n",
    "        observation = torch.from_numpy(observation).float().unsqueeze(0)\n",
    "        probabilities = self.policy(observation)\n",
    "        \n",
    "        m = Categorical(probabilities)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        self.log_action_probabilities.append(log_prob)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def get_policy(self) -> nn.Module:\n",
    "        inputs_no = self.get_observations()\n",
    "        outputs_no = self.get_actions()\n",
    "        discrete_outputs = self.discrete_actions\n",
    "        return Policy(inputs_no, outputs_no, discrete_outputs)\n",
    "    \n",
    "    def update_policy(self, rewards: torch.tensor, *args, **kwargs):\n",
    "        del args, kwargs  # unused\n",
    "        \n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        returns = self.calculate_returns(rewards)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(self.log_action_probabilities, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "                    \n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        del self.log_action_probabilities[:]\n",
    "            \n",
    "    def calculate_returns(self, rewards: torch.tensor) -> torch.tensor:\n",
    "        returns = torch.flip(rewards, [0])\n",
    "        for idx, item in enumerate(returns):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            returns[idx] = item + self.gamma * returns[idx - 1]\n",
    "        return torch.flip(returns, [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the environmnet and the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = PolicyAgent(env, gamma=0.9, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(agent: agent_base.Agent, env: gym.Env) -> Tuple[agent_base.Agent, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform one simulation (episode) to collect data for the policy update.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    rewards = torch.Tensor(0, 1).float()\n",
    "    \n",
    "    for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "        action = agent(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        rewards = torch.cat([rewards, torch.tensor(reward).view(1, 1)])\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    return agent, rewards\n",
    "            \n",
    "\n",
    "def print_epiode_stats(i_episode: int, epiosde_reward: float, running_reward: float, \n",
    "                       env: gym.Env, rewards: torch.Tensor):\n",
    "    if i_episode % 50 == 0:\n",
    "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}\\t'.format(\n",
    "              i_episode, ep_reward, running_reward))\n",
    "        \n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        t = rewards.size()[0]\n",
    "        print(\"Solved - epiosde {}! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\".format(i_episode, running_reward, t))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast reward: 10.00\tAverage reward: 10.00\t\n",
      "Episode 50\tLast reward: 62.00\tAverage reward: 35.96\t\n",
      "Episode 100\tLast reward: 38.00\tAverage reward: 75.04\t\n",
      "Episode 150\tLast reward: 54.00\tAverage reward: 133.00\t\n",
      "Episode 200\tLast reward: 500.00\tAverage reward: 366.23\t\n",
      "Episode 250\tLast reward: 233.00\tAverage reward: 287.45\t\n",
      "Episode 300\tLast reward: 97.00\tAverage reward: 233.20\t\n",
      "Episode 350\tLast reward: 94.00\tAverage reward: 119.48\t\n",
      "Episode 400\tLast reward: 347.00\tAverage reward: 299.01\t\n",
      "Episode 450\tLast reward: 227.00\tAverage reward: 316.72\t\n",
      "Episode 500\tLast reward: 119.00\tAverage reward: 249.83\t\n",
      "Episode 550\tLast reward: 171.00\tAverage reward: 176.32\t\n",
      "Episode 600\tLast reward: 161.00\tAverage reward: 146.63\t\n",
      "Episode 650\tLast reward: 223.00\tAverage reward: 228.86\t\n",
      "Episode 700\tLast reward: 500.00\tAverage reward: 376.35\t\n",
      "Episode 750\tLast reward: 190.00\tAverage reward: 228.14\t\n",
      "Episode 800\tLast reward: 500.00\tAverage reward: 443.07\t\n",
      "Episode 850\tLast reward: 172.00\tAverage reward: 282.47\t\n",
      "Episode 900\tLast reward: 267.00\tAverage reward: 417.58\t\n",
      "Episode 950\tLast reward: 118.00\tAverage reward: 227.04\t\n",
      "Episode 1000\tLast reward: 145.00\tAverage reward: 149.41\t\n",
      "Episode 1050\tLast reward: 244.00\tAverage reward: 400.69\t\n",
      "Episode 1100\tLast reward: 350.00\tAverage reward: 236.95\t\n",
      "Episode 1150\tLast reward: 119.00\tAverage reward: 244.13\t\n",
      "Episode 1200\tLast reward: 201.00\tAverage reward: 171.36\t\n",
      "Episode 1250\tLast reward: 114.00\tAverage reward: 158.23\t\n",
      "Episode 1300\tLast reward: 168.00\tAverage reward: 152.85\t\n",
      "Episode 1350\tLast reward: 165.00\tAverage reward: 188.15\t\n",
      "Episode 1400\tLast reward: 335.00\tAverage reward: 187.30\t\n",
      "Episode 1450\tLast reward: 97.00\tAverage reward: 266.38\t\n",
      "Episode 1500\tLast reward: 304.00\tAverage reward: 204.91\t\n",
      "Episode 1550\tLast reward: 377.00\tAverage reward: 213.93\t\n",
      "Episode 1600\tLast reward: 114.00\tAverage reward: 166.90\t\n",
      "Episode 1650\tLast reward: 67.00\tAverage reward: 102.29\t\n",
      "Episode 1700\tLast reward: 103.00\tAverage reward: 96.08\t\n",
      "Episode 1750\tLast reward: 132.00\tAverage reward: 141.73\t\n",
      "Episode 1800\tLast reward: 113.00\tAverage reward: 184.53\t\n",
      "Episode 1850\tLast reward: 152.00\tAverage reward: 141.10\t\n",
      "Episode 1900\tLast reward: 164.00\tAverage reward: 153.12\t\n",
      "Episode 1950\tLast reward: 110.00\tAverage reward: 131.11\t\n",
      "Episode 2000\tLast reward: 154.00\tAverage reward: 143.90\t\n",
      "Episode 2050\tLast reward: 141.00\tAverage reward: 142.51\t\n",
      "Episode 2100\tLast reward: 120.00\tAverage reward: 131.55\t\n",
      "Episode 2150\tLast reward: 110.00\tAverage reward: 111.69\t\n",
      "Episode 2200\tLast reward: 179.00\tAverage reward: 151.49\t\n",
      "Episode 2250\tLast reward: 77.00\tAverage reward: 128.79\t\n",
      "Episode 2300\tLast reward: 105.00\tAverage reward: 105.83\t\n",
      "Episode 2350\tLast reward: 209.00\tAverage reward: 164.27\t\n",
      "Episode 2400\tLast reward: 213.00\tAverage reward: 165.94\t\n",
      "Episode 2450\tLast reward: 138.00\tAverage reward: 146.24\t\n",
      "Episode 2500\tLast reward: 85.00\tAverage reward: 118.42\t\n",
      "Episode 2550\tLast reward: 195.00\tAverage reward: 138.97\t\n",
      "Episode 2600\tLast reward: 132.00\tAverage reward: 122.05\t\n",
      "Episode 2650\tLast reward: 49.00\tAverage reward: 96.70\t\n",
      "Episode 2700\tLast reward: 156.00\tAverage reward: 132.21\t\n",
      "Episode 2750\tLast reward: 167.00\tAverage reward: 159.33\t\n",
      "Episode 2800\tLast reward: 230.00\tAverage reward: 187.21\t\n",
      "Episode 2850\tLast reward: 209.00\tAverage reward: 321.00\t\n",
      "Episode 2900\tLast reward: 149.00\tAverage reward: 177.99\t\n",
      "Episode 2950\tLast reward: 210.00\tAverage reward: 162.26\t\n",
      "Episode 3000\tLast reward: 167.00\tAverage reward: 170.16\t\n",
      "Episode 3050\tLast reward: 224.00\tAverage reward: 181.96\t\n",
      "Episode 3100\tLast reward: 163.00\tAverage reward: 141.37\t\n",
      "Episode 3150\tLast reward: 192.00\tAverage reward: 156.09\t\n",
      "Episode 3200\tLast reward: 277.00\tAverage reward: 181.71\t\n",
      "Episode 3250\tLast reward: 500.00\tAverage reward: 423.16\t\n",
      "Solved - epiosde 3272! Running reward is now 475.13835978853746 and the last episode runs to 500 time steps!\n"
     ]
    }
   ],
   "source": [
    "running_reward = 10\n",
    "for i_episode in range(4000):\n",
    "    agent, rewards = run_episode(agent, env)\n",
    "    ep_reward = rewards.sum().item()\n",
    "    \n",
    "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    print_epiode_stats(i_episode, ep_reward, running_reward, env, rewards)\n",
    "        \n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        break\n",
    "\n",
    "    agent.update_policy(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "for i in range(300):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    action = agent(obs)\n",
    "    obs, rew, done, info = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
